{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxinet Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the taxinet data we generated from the last exercise to train a neural network to estimate our current crosstrack error and heading error from a downsampled runway image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets import some of the python packages we will need. One of the packages we will be importing from in [tensorflow](https://www.tensorflow.org/). Tensorflow is a very popular machine learning package that implements a lot of the functions we need for training neural networks for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data ready\n",
    "\n",
    "We have generated a dataset 20,000 downsampled images using the code from the data generation notebook for you. The data is stored in the file called `taxi_ai4all_data.h5`. Run the following cell to read all of the training images into a numpy array called `X` and all of the labels into a numpy array called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('taxi_ai4all_data.h5', 'r')\n",
    "X = np.array(f['X'])\n",
    "y = np.array(f['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether we are overfitting, we need to create a separate training and test set from our data. We will use the first 18,000 images as our training set and set aside the last 2000 images as the test set. Run the cell below to split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train = X[:, :18000]\n",
    "y_train = y[:, :18000]\n",
    "\n",
    "X_test = X[:, 18000:]\n",
    "y_test = y[:, 18000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your network architecture\n",
    "\n",
    "Next, we will start selecting some of the **hyperparameters** of our network such as the number of layers and the sizes of the layers. The function below takes in a list of layer sizes and returns a tensorflow neural network model with the corresponding network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer_sizes):\n",
    "    model = Sequential()\n",
    "    for i in range(1, len(layer_sizes) - 1):\n",
    "        model.add(Dense(layer_sizes[i], activation = 'relu'))\n",
    "    model.add(Dense(layer_sizes[-1]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the cell below with your desired layer sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [] # Fill in your desired layer sizes!\n",
    "model = build_model(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your optimizer\n",
    "\n",
    "We will use tensorflow \"stochastic gradient descent\" or SGD optimizer which work similarly to the gradient descent optimizer we learned about earlier! Fill in the cell below with your learning rate to create the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = None # Fill in your learning rate!\n",
    "opt = SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model with your loss function\n",
    "\n",
    "The following cell tells tensorflow to use our SGD optimizer with a mean squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!\n",
    "\n",
    "Run the cell below to train the model. The training will run for 100 epochs. You can think of an epoch like an iteration of gradient descent. At each epoch, tensorflow will print out our training and validation (test) loss. Before running the cell, discuss with your group what should happen to these values during training.\n",
    "\n",
    "Once you are ready, run the cell to train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train.T, y_train.T, epochs=100, batch_size=64, validation_data=(X_test.T, y_test.T), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's see how our training did! Run the cell below to get our model's predictions on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train.T)\n",
    "y_test_pred = model.predict(X_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check out our model evaluations on the first 10 images in our test set. Run the cell create a plot with this information. This might be a nice plot to include in your final presentation :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2, 5, i + 1)\n",
    "    img = np.reshape(X_test[:, i], (8, 16))\n",
    "    imgplot = plt.imshow(img, cmap='gray', clim=(0, 1))\n",
    "    title_string = 'True: cte = ' + \\\n",
    "                    str(round(y_test[0, i], 3)) +  \\\n",
    "                    ', he = ' + \\\n",
    "                    str(round(y_test[1, i], 3)) + \\\n",
    "                    '\\nPred: cte = ' + \\\n",
    "                    str(round(y_test_pred[i, 0], 3)) +  \\\n",
    "                    ', he = ' + \\\n",
    "                    str(round(y_test_pred[i, 1], 3))\n",
    "    ax.set_title(title_string)\n",
    "    plt.axis('off')\n",
    "\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it do on these examples? Looking at predictions on a few examples can give you an idea of whether your model is working, but in order to get the whole picture, we want to analyze some metrics across all of the examples in our data. To do that, let's compute the absolute value of the prediction errors for the train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_train = np.abs(y_train_pred.T - y_train)\n",
    "errors_test = np.abs(y_test_pred.T - y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we analyze all 20,000 errors at once? A histogram is a good way to visualize the distribution of a set of data, so let's try to create one of those using matplotlib's `hist` function. Run the cell below to plot the errors on the training and test set. (Another plot this might be good for your final presentation :))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(errors_train[0, :])\n",
    "plt.xlabel('Absolute Value of the Error')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Training CTE Error Distribution')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(errors_test[0, :])\n",
    "plt.xlabel('Absolute Value of the Error')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Testing CTE Error Distribution')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(errors_train[1, :])\n",
    "plt.xlabel('Absolute Value of the Error')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Training HE Error Distribution')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(errors_test[1, :])\n",
    "plt.xlabel('Absolute Value of the Error')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Testing HE Error Distribution')\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the two distributions. Is it what you expect? Discuss with your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results and make changes\n",
    "\n",
    "Now that you have trained a network and done some analysis, it's time to go back and try to make some changes to improve your model. Try tuning some of the **hyperparameters** to observe their affect on the training and the effectiveness of the final model. Some hyperparameters to try changing include:\n",
    "- number of layers\n",
    "- number of neurons in each layer\n",
    "- learning rate\n",
    "- number of epochs\n",
    "- batch size\n",
    "\n",
    "As you go back through the training cycle and test out your changes, discuss the following questions with your group. When we come back together, we will ask each group to share their findings.\n",
    "1. How do the number of layers and the number of neurons in each layer seem to affect the training?\n",
    "2. How does changing the learning rate affect the training?\n",
    "3. Does increasing the number of epochs help? Could we decrease it?\n",
    "4. Do the train and validation (test accuracy) seem to match each other closely? If so, why? If not, why not?\n",
    "\n",
    "In order to effectively control the aircraft, you should try to get your mse below 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for later!\n",
    "Once you are satisfied with your model, run the following line to save the model to a file called `Taxinet.h5`. This will allow us to use it in the next notebook where we make it taxi down the runway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Taxinet.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}